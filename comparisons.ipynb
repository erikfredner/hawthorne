{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus comparisons\n",
    "Evaluating the difference Hawthorne's work makes between the corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean sentence length\n",
    "This exemplifies how similar Hawthorne is to the rest of the corpus, as well as his small impact on corpus-wide measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import statistics\n",
    "import re\n",
    "import os\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hawthorne's sentences\n",
    "For this test, I use the Library of America Hawthorne, as it is better transcribed than Gale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh_loa = '/Users/e/code/hawthorne/local/loa_nh/loa_hawthorne_all.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_lens(file):\n",
    "    with open(file) as f:\n",
    "        text = f.read()\n",
    "        sents = nltk.tokenize.sent_tokenize(text)\n",
    "        \n",
    "    sent_lens = []\n",
    "\n",
    "    for sent in sents:\n",
    "        n_toks = len([x for x in re.split('\\s+', sent) if x])\n",
    "        sent_lens.append(n_toks)\n",
    "    \n",
    "    return sent_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh_sents = get_sent_lens(nh_loa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.08811897217661"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.mean(nh_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.0, 20.0, 34.0]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.quantiles(nh_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hawthorne's peers' sentences\n",
    "Comparing Hawthorne's sentence lengths against those of his contemporaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = '/Users/e/code/hawthorne/local/corpus_no_nh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [os.path.join(comparison, x) for x in os.listdir(comparison) if x.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(files) # randomize sample\n",
    "sample_size = round(len(files) * 0.1) # get n for 10% sample\n",
    "sample = files[:sample_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_lens = []\n",
    "\n",
    "for i, file in enumerate(sample):\n",
    "    sent_lens.extend(get_sent_lens(file))\n",
    "    \n",
    "    if i % 25 == 0:\n",
    "        print('\\r{} of {}'.format(i, len(sample)), end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.374018741025175"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.mean(sent_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11.0, 21.0, 35.0]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.quantiles(sent_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectors\n",
    "Here, I use the [hyperhyper](https://github.com/jfilter/hyperhyper) implementation of the SVD PPMI method of creating word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperhyper as hy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hawthorne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh_corpus = '/Users/e/code/hawthorne/local/gale_nh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build up vocab: 100%|██████████| 23/23 [00:03<00:00,  7.54it/s]\n",
      "texts to ids: 100%|██████████| 23/23 [00:02<00:00, 10.55it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate\n",
    "# corpus = hy.Corpus.from_text_files(nh_corpus, preproc_func = hy.tokenize_texts, keep_n = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate\n",
    "# bunch = hy.Bunch(\"results/hy_hawthorne\", corpus, force_overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "bunch = hy.Bunch('results/hy_hawthorne/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_nh, results_nh = bunch.svd(keyed_vectors = True, subsample = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prynne', 0.6416890621185303),\n",
       " ('pearl', 0.24967367947101593),\n",
       " ('miriam', 0.23768633604049683),\n",
       " ('she', 0.22650045156478882),\n",
       " ('hibbins', 0.2198188453912735),\n",
       " ('thou', 0.19863906502723694),\n",
       " ('mistress', 0.19824433326721191),\n",
       " ('thyself', 0.19154658913612366),\n",
       " ('pit', 0.1914704144001007),\n",
       " ('arthur', 0.18614283204078674)]"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_nh.most_similar('hester')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('child', 0.25472408533096313),\n",
       " ('hester', 0.24967367947101593),\n",
       " ('mother', 0.2277851104736328),\n",
       " ('naughty', 0.19791392982006073),\n",
       " ('hannah', 0.1876162588596344),\n",
       " ('babe', 0.17599552869796753),\n",
       " ('she', 0.17360076308250427),\n",
       " ('rigby', 0.1705891191959381),\n",
       " ('listened', 0.1696358472108841),\n",
       " ('hush', 0.16588468849658966)]"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_nh.most_similar('pearl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gale 1828-1864 including NH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/e/code/hawthorne/local/corpus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate\n",
    "# corpus = hy.Corpus.from_text_files(path, preproc_func = hy.tokenize_texts, keep_n = 10000)\n",
    "\n",
    "# load\n",
    "corpus = hy.Corpus.load('results/corpus_subsample_none/corpus.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate\n",
    "# bunch = hy.Bunch(\"results/corpus_subsample_none\", corpus, force_overwrite = True)\n",
    "\n",
    "# load\n",
    "bunch = hy.Bunch('results/corpus_subsample_none//')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors, results = bunch.svd(keyed_vectors = True, subsample = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('methodist', 0.3934868574142456),\n",
       " ('primitive', 0.3582947552204132),\n",
       " ('dutch', 0.35332465171813965),\n",
       " ('quaker', 0.33089378476142883),\n",
       " ('protestant', 0.3207319974899292),\n",
       " ('baptist', 0.3153371214866638),\n",
       " ('catholic', 0.3104977011680603),\n",
       " ('jewish', 0.24452072381973267),\n",
       " ('christian', 0.23519401252269745),\n",
       " ('parson', 0.22869554162025452)]"
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.most_similar('puritan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('miriam', 0.3072657883167267),\n",
       " ('isabel', 0.303394615650177),\n",
       " ('christine', 0.29878801107406616),\n",
       " ('maud', 0.27142563462257385),\n",
       " ('constance', 0.2633606493473053),\n",
       " ('marian', 0.26311105489730835),\n",
       " ('theresa', 0.25412988662719727),\n",
       " ('annie', 0.2525578439235687),\n",
       " ('pauline', 0.2524963617324829),\n",
       " ('linda', 0.25059691071510315)]"
      ]
     },
     "execution_count": 727,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.most_similar('hester')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gale 1828-1864 excluding NH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/e/code/hawthorne/local/corpus_no_nh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate\n",
    "# corpus_no_nh = hy.Corpus.from_text_files(path, preproc_func = hy.tokenize_texts, keep_n = 10000)\n",
    "\n",
    "# load\n",
    "corpus_no_nh = hy.Corpus.load('results/corpus_no_nh_subsample_none/corpus.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate\n",
    "# bunch = hy.Bunch(\"results/corpus_no_nh_subsample_none\", corpus, force_overwrite=True)\n",
    "\n",
    "# load\n",
    "bunch = hy.Bunch('results/corpus_no_nh_subsample_none//')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_no_nh, results_no_nh = bunch.svd(keyed_vectors = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('methodist', 0.34290313720703125),\n",
       " ('quaker', 0.3168374300003052),\n",
       " ('primitive', 0.29920345544815063),\n",
       " ('dutch', 0.29672470688819885),\n",
       " ('baptist', 0.2812960147857666),\n",
       " ('catholic', 0.27922335267066956),\n",
       " ('english', 0.2629094123840332),\n",
       " ('protestant', 0.2529698610305786),\n",
       " ('christian', 0.24499836564064026),\n",
       " ('yankee', 0.23915767669677734)]"
      ]
     },
     "execution_count": 652,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_no_nh.most_similar('puritan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('julie', 0.2691107392311096),\n",
       " ('christine', 0.2456360012292862),\n",
       " ('anna', 0.2245090901851654),\n",
       " ('estelle', 0.21444502472877502),\n",
       " ('viola', 0.20676776766777039),\n",
       " ('julia', 0.20160716772079468),\n",
       " ('emma', 0.20148344337940216),\n",
       " ('adeline', 0.19821283221244812),\n",
       " ('theresa', 0.1944657564163208),\n",
       " ('apd', 0.1920011192560196)]"
      ]
     },
     "execution_count": 724,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_no_nh.most_similar('hester')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing vectors across models\n",
    "In order to compare across models, we take aggregate the differences of the cosine similarities between every vector in the model.\n",
    "\n",
    "Because SVDs of PPMI are deterministic, differences between the two models are attributable to the presence or absence of Hawthorne's work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "import pickle\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(c):\n",
    "    '''\n",
    "    Gets vocabulary from hyperhyper corpus object, c.\n",
    "    '''\n",
    "    return set([x for x in c.vocab.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shared_vocab(c1, c2):\n",
    "    '''\n",
    "    Returns shared vocab between hyperhyper corpus objects.\n",
    "    '''\n",
    "    return list(get_vocab(c1) & get_vocab(c2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_vector(word, v1, v2, vocab = None, topn = 10000, pkl = False):\n",
    "    '''\n",
    "    Takes hyperhyper vector objects and returns the difference between those vectors as a dict.\n",
    "    '''\n",
    "    # get cosine similarities\n",
    "    sim1 = v1.most_similar(word, topn = topn)\n",
    "    sim2 = v2.most_similar(word, topn = topn)\n",
    "\n",
    "    # calculate differences\n",
    "    sim1 = pd.Series(data = [x[1] for x in sim1], index = [x[0] for x in sim1])\n",
    "    sim2 = pd.Series(data = [x[1] for x in sim2], index = [x[0] for x in sim2])\n",
    "    diff = sim1 - sim2\n",
    "    total = diff.abs().sum()\n",
    "    \n",
    "    # optionally pickle output\n",
    "    if pkl:\n",
    "        d = {'word'          : word,\n",
    "             'abs_diff'      : total,\n",
    "             'sim1'          : sim1,\n",
    "             'sim2'          : sim2}\n",
    "        \n",
    "        fn = str(round(time.time())) + '_' + word + '.pkl'\n",
    "        path = os.path.join('results/vec_diffs', fn)\n",
    "\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(d, f)\n",
    "            f.close()\n",
    "        print('\\r{} pickled at {}'.format(word, path))\n",
    "    \n",
    "    # return observation\n",
    "    d = {'word'          : word,\n",
    "         'abs_diff'      : total}\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear (takes 5-10 minutes)\n",
    "\n",
    "def diff_vectors(v1, v2, vocab):\n",
    "    '''\n",
    "    Takes hyperhyper vector objects (v1, v2), and calculates the absolute difference between them.\n",
    "    \n",
    "    Corpus objects (c1, c2) used to extract vocabularies and calculate intersection.\n",
    "    '''\n",
    "    \n",
    "    l = []\n",
    "    \n",
    "    for i, word in enumerate(vocab):\n",
    "        l.append(diff_vector(word, v1, v2))\n",
    "        if i % 100 == 0:\n",
    "            pct = round((i / len(vocab)) * 100)\n",
    "            print('\\r{}%'.format(pct), end = '')\n",
    "    \n",
    "    # pickle\n",
    "    fn = str(round(time.time())) + '_diff_vectors.pkl'\n",
    "    path = os.path.join('results/vec_diffs', fn)\n",
    "    \n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(l, f)\n",
    "        f.close()\n",
    "    print('\\rpickle: {}'.format(path))\n",
    "            \n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_shared_vocab(corpus, corpus_no_nh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickle: results/vec_diffs/1605914440_diff_vectors.pkl\n"
     ]
    }
   ],
   "source": [
    "# generate\n",
    "diffs = diff_vectors(vectors, vectors_no_nh, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Munge results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "diffs = pickle.load(open('/Users/e/code/hawthorne/results/vec_diffs/1605914440_diff_vectors.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    9985.000000\n",
       "mean      213.882300\n",
       "std        27.214233\n",
       "min       119.660099\n",
       "25%       194.109672\n",
       "50%       211.449624\n",
       "75%       232.322910\n",
       "max       350.309245\n",
       "Name: abs_diff, dtype: float64"
      ]
     },
     "execution_count": 668,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['abs_diff'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add corpus raw frequency to output\n",
    "d = {v : corpus.counts[k] for k,v in corpus.vocab.items()}\n",
    "e = {v : corpus_no_nh.counts[k] for k,v in corpus_no_nh.vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make pandas objects\n",
    "ds = pd.Series(d)\n",
    "ds.name = '# Gale'\n",
    "es = pd.Series(e)\n",
    "es.name = '# Gale - NH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.merge(ds, es, left_on = ds.index, right_on = es.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.rename(columns = {'key_0':'word'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, counts, on = 'word').set_index('word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abs_diff</th>\n",
       "      <th># Gale</th>\n",
       "      <th># Gale - NH</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>twice</th>\n",
       "      <td>234.924290</td>\n",
       "      <td>11751</td>\n",
       "      <td>11694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>muscle</th>\n",
       "      <td>206.839319</td>\n",
       "      <td>2026</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idiot</th>\n",
       "      <td>226.529023</td>\n",
       "      <td>1520</td>\n",
       "      <td>1509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trio</th>\n",
       "      <td>223.319823</td>\n",
       "      <td>1318</td>\n",
       "      <td>1317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>again</th>\n",
       "      <td>230.995042</td>\n",
       "      <td>194486</td>\n",
       "      <td>193522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          abs_diff  # Gale  # Gale - NH\n",
       "word                                   \n",
       "twice   234.924290   11751        11694\n",
       "muscle  206.839319    2026         2021\n",
       "idiot   226.529023    1520         1509\n",
       "trio    223.319823    1318         1317\n",
       "again   230.995042  194486       193522"
      ]
     },
     "execution_count": 677,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hawthorne accounts for about 1 million words (`1062724`) in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh_n = df['# Gale'].sum() - df['# Gale - NH'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1062724"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nh_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In percentage terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.512%'"
      ]
     },
     "execution_count": 678,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'{}%'.format(round((nh_n / df['# Gale'].sum()) * 100, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-word proportion\n",
    "observed = 1 - (df['# Gale - NH'] / df['# Gale'])\n",
    "expected = nh_n / df['# Gale'].sum()\n",
    "df['obs / exp'] = observed / expected\n",
    "\n",
    "# Create status for obs/exp for plotting\n",
    "df['o/e labels'] = ['more' if x > 1 else 'less' for x in df['obs / exp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from David Bamman's anlp19\n",
    "import operator\n",
    "\n",
    "def chi_square(one_counts, two_counts):\n",
    "\n",
    "    one_sum=0.\n",
    "    two_sum=0.\n",
    "    vocab={}\n",
    "    for word in one_counts.index:\n",
    "        one_sum+=one_counts[word]\n",
    "        vocab[word]=1\n",
    "    for word in two_counts.index:\n",
    "        vocab[word]=1\n",
    "        two_sum+=two_counts[word]\n",
    "\n",
    "    N=one_sum+two_sum\n",
    "    vals={}\n",
    "    \n",
    "    for word in vocab:\n",
    "        O11=one_counts[word]\n",
    "        O12=two_counts[word]\n",
    "        O21=one_sum-one_counts[word]\n",
    "        O22=two_sum-two_counts[word]\n",
    "        \n",
    "        # We'll use the simpler form given in Manning and Schuetze (1999) \n",
    "        # for 2x2 contingency tables: \n",
    "        # https://nlp.stanford.edu/fsnlp/promo/colloc.pdf, equation 5.7\n",
    "        \n",
    "        vals[word]=(N*(O11*O22 - O12*O21)**2)/((O11 + O12)*(O11+O21)*(O12+O22)*(O21+O22))\n",
    "        \n",
    "    sorted_chi = sorted(vals.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    one=[]\n",
    "    two=[]\n",
    "    \n",
    "    for k,v in sorted_chi:\n",
    "        if one_counts[k]/one_sum > two_counts[k]/two_sum:\n",
    "            one.append(k)\n",
    "        else:\n",
    "            two.append(k)\n",
    "        \n",
    "    return one, vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dws, vals = chi_square(df['# Gale'], df['# Gale - NH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per chi2\n",
    "df['nh_distinctive'] = df.index.isin(dws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['chi2'] = pd.Series(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['n_diff'] = df['# Gale'] - df['# Gale - NH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add stopword metadata\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "df['stopword'] = df.index.isin(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary validation\n",
    "with open('/Users/e/Documents/Literary Lab/word lists/oed_wordlist.txt', 'r') as f:\n",
    "    oed_words = [x for x in f.read().split('\\n') if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dict_word'] = df.index.isin(oed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the dataset used to produce graphs in Tableau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/Users/e/code/hawthorne/hawthorne_diffs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating dissimilarity by rank change\n",
    "How does Hawthorne re-rank vector similarities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected from Tableau\n",
    "keywords = ['likewise', 'puritan', 'minister', 'artist', 'substance', 'methinks', 'whether', 'lifetime',\n",
    "           'world', 'mankind', 'pilgrims', 'fling', 'province', 'painter', 'brotherhood', 'clergyman',\n",
    "           'artist', 'fireside', 'might', 'hither', 'reverend', 'whatever', 'merely', 'fireside', 'antique',\n",
    "           'wrought', 'airy', 'discern', 'shadow', 'image', 'actual', 'flung', 'dusky']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_similarities(word, vectors):\n",
    "    c = 1\n",
    "    d = {}\n",
    "    \n",
    "    for x in vectors.most_similar(word, topn=10000):\n",
    "        d[x[0]] = c\n",
    "        c += 1\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rank_change(word, v1, v2, topn = 20, spearman = True):\n",
    "    d = rank_similarities(word, v1)\n",
    "    e = rank_similarities(word, v2)\n",
    "    data = pd.concat([pd.Series(d), pd.Series(e)], axis = 1)\n",
    "    data.columns = ['v1_rank', 'v2_rank']\n",
    "    data['rank_change'] = data['v1_rank'] - data['v2_rank']\n",
    "    # filter\n",
    "    data.dropna(axis=0, inplace=True)\n",
    "    subset = data[data['v1_rank'] <= topn]\n",
    "    subset.sort_values('rank_change', ascending = False, inplace = True)\n",
    "    # write\n",
    "    if spearman:\n",
    "        sp = stats.spearmanr(subset['v1_rank'], subset['v2_rank'])[0]\n",
    "        fp = '/Users/e/code/hawthorne/results/rank_changes'\n",
    "        fp = os.path.join(fp, '{}_{}_rank_change.csv'.format(str(sp)[:4], word))\n",
    "        subset.to_csv(fp)\n",
    "    else:\n",
    "        fp = '/Users/e/code/hawthorne/results/rank_changes'\n",
    "        fp = os.path.join(fp, '{}_rank_change.csv'.format(word))\n",
    "    # notify\n",
    "    print(fp)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-810-1be216334ace>:10: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/e/code/hawthorne/results/rank_changes/0.54_likewise_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.76_puritan_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.80_minister_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.68_artist_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.53_substance_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.61_methinks_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.58_whether_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.64_lifetime_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.66_world_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.56_mankind_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.70_pilgrims_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.74_fling_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.86_province_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.81_painter_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.61_brotherhood_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.81_clergyman_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.68_artist_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.85_fireside_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.81_might_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.80_hither_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.72_reverend_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.74_whatever_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.74_merely_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.85_fireside_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.74_antique_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.56_wrought_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.67_airy_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.77_discern_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.79_shadow_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.85_image_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.76_actual_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.78_flung_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.80_dusky_rank_change.csv\n"
     ]
    }
   ],
   "source": [
    "for word in keywords:\n",
    "    get_rank_change(word, vectors_no_nh, vectors, topn = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/e/code/hawthorne/results/rank_changes/0.95_five_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.91_six_rank_change.csv\n",
      "/Users/e/code/hawthorne/results/rank_changes/0.88_twenty_rank_change.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-810-1be216334ace>:10: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testing low volatility words\n",
    "for word in ['five', 'six', 'twenty']:\n",
    "    get_rank_change(word, vectors_no_nh, vectors, topn = 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
